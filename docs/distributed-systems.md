# 13. 분산 시스템 (Distributed Systems)

분산 시스템은 **네트워크로 연결된 여러 컴퓨터가 협력하여 하나의 시스템처럼 동작**하는 것을 말한다. 우리가 매일 사용하는 Google, Netflix, 카카오톡 같은 서비스들은 모두 분산 시스템이다.

왜 분산 시스템이 필요한가? 단일 서버로는 수억 명의 사용자를 감당할 수 없기 때문이다. 하지만 여러 컴퓨터를 연결하면 새로운 문제들이 발생한다: 네트워크 장애, 데이터 불일치, 부분 실패 등. 분산 시스템의 핵심은 이러한 문제들을 어떻게 해결하느냐이다.

---

## 1. 분산 시스템 기초 ◑

### 왜 분산 시스템을 사용하는가? ◑

**단일 서버의 한계**

```
단일 서버:
┌─────────────────────────────────┐
│                                 │
│   CPU: 128코어 (물리적 한계)     │
│   RAM: 12TB (물리적 한계)        │
│   Storage: 수백 TB              │
│                                 │
│   처리량: 초당 10만 요청 (한계)   │
│                                 │
└─────────────────────────────────┘
         ↑
    수백만 사용자가 동시 접속하면?
    → 서버가 감당 불가
```

**분산 시스템의 해결책**

```
분산 시스템:
┌──────────┐  ┌──────────┐  ┌──────────┐
│ Server 1 │  │ Server 2 │  │ Server 3 │  ... (필요한 만큼 추가)
└────┬─────┘  └────┬─────┘  └────┬─────┘
     │             │             │
     └─────────────┼─────────────┘
                   │
              [로드 밸런서]
                   │
              수백만 사용자

→ 서버를 계속 추가하여 거의 무한히 확장 가능
```

### 분산 시스템의 특징과 도전 ◑

| 특징 | 설명 | 도전 과제 |
|------|------|----------|
| **확장성** | 노드를 추가하여 성능 향상 | 데이터 분산, 로드 밸런싱 |
| **장애 허용** | 일부 노드 장애에도 시스템 동작 | 장애 감지, 자동 복구 |
| **투명성** | 사용자에게 단일 시스템처럼 보임 | 복잡한 내부 구조 숨김 |
| **지리적 분산** | 전 세계에 서버 배치 가능 | 네트워크 지연, 데이터 동기화 |

### 분산 시스템의 핵심 도전: 부분 실패 ◑

분산 시스템에서 가장 어려운 점은 **부분 실패(Partial Failure)** 이다.

```
단일 시스템:
- 서버가 동작하거나, 죽거나 (두 가지 상태)
- 명확한 상태 파악 가능

분산 시스템:
- 서버 A는 정상, 서버 B는 느림, 서버 C는 응답 없음
- "응답 없음"의 의미가 불명확:
  ├── 서버가 죽었나?
  ├── 네트워크가 끊겼나?
  ├── 응답이 느릴 뿐인가?
  └── 요청은 처리했는데 응답만 유실됐나?
```

이런 불확실성 때문에 분산 시스템은 복잡해진다.

### 수평 확장 vs 수직 확장 ◑

시스템의 처리 능력을 높이는 두 가지 방법이 있다.

**수직 확장 (Scale Up)**: 한 서버를 더 강하게
```
Before:              After:
┌─────────────┐      ┌─────────────┐
│ CPU: 4코어  │  →   │ CPU: 32코어 │
│ RAM: 16GB   │      │ RAM: 256GB  │
│ SSD: 500GB  │      │ SSD: 4TB    │
└─────────────┘      └─────────────┘

장점: 단순함, 애플리케이션 수정 불필요
단점: 비용이 기하급수적으로 증가, 물리적 한계 존재
```

**수평 확장 (Scale Out)**: 서버를 더 많이
```
Before:              After:
┌─────────────┐      ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐
│ Server 1    │  →   │ S1   │ │ S2   │ │ S3   │ │ S4   │
└─────────────┘      └──────┘ └──────┘ └──────┘ └──────┘

장점: 비용이 선형적, 거의 무한히 확장 가능
단점: 복잡함, 분산 처리 로직 필요
```

**실제 선택 기준**
- 초기 스타트업: 수직 확장으로 시작 (단순함)
- 대규모 서비스: 수평 확장 필수 (Netflix, Google 등)
- 대부분의 경우: 둘을 조합하여 사용

---

## 2. CAP 정리 ◑

### CAP 정리란? ◑

2000년 Eric Brewer가 제안한 정리로, 분산 시스템이 **세 가지 속성을 동시에 모두 만족할 수 없다**는 것이다.

```
                    Consistency
                    (일관성)
                        △
                       / \
                      /   \
                     /     \
                    /   ?   \
                   /         \
                  /___________\
        Availability    Partition Tolerance
          (가용성)         (분할 내성)

→ 세 꼭짓점을 모두 만족하는 것은 불가능
→ 최대 두 가지만 선택 가능
```

### 세 가지 속성 상세 설명 ◑

**1. Consistency (일관성)**

모든 노드가 **같은 시점에 같은 데이터**를 본다.

```
일관성 있는 시스템:
┌─────────┐        ┌─────────┐
│ Node A  │        │ Node B  │
│ x = 5   │        │ x = 5   │
└─────────┘        └─────────┘

사용자 1이 Node A에 x = 10 쓰기 →
사용자 2가 Node B에서 x 읽기 → 반드시 10을 읽음

일관성 없는 시스템 (Eventual Consistency):
사용자 2가 Node B에서 x 읽기 → 5를 읽을 수도 있음 (아직 동기화 안 됨)
```

**2. Availability (가용성)**

모든 요청이 **성공 또는 실패 응답**을 받는다. 시스템이 항상 응답한다.

```
가용성 있는 시스템:
요청 → 응답 (성공 또는 실패, 하지만 반드시 응답)

가용성 없는 시스템:
요청 → ... (응답 없음, 타임아웃)
```

**3. Partition Tolerance (분할 내성)**

네트워크가 **분할(파티션)** 되어도 시스템이 계속 동작한다.

```
네트워크 분할 상황:

┌─────────┐                    ┌─────────┐
│ Node A  │ ═══════╳═══════════│ Node B  │
│         │   네트워크 단절     │         │
└─────────┘                    └─────────┘
     ↑                              ↑
  사용자 1                       사용자 2

두 노드가 서로 통신 불가
→ 분할 내성이 있으면: 각 노드가 독립적으로 서비스 계속
→ 분할 내성이 없으면: 시스템 중단
```

### ⭕ CAP 정리: 왜 세 가지를 동시에 만족할 수 없는가?

네트워크 분할은 **분산 시스템에서 불가피**하다. 아무리 좋은 네트워크도 언젠가는 문제가 생긴다. 따라서 **P(분할 내성)는 포기할 수 없다**.

결국 실제 선택은 **C(일관성) vs A(가용성)** 이다.

```
네트워크 분할 발생 시:

┌─────────┐         ╳         ┌─────────┐
│ Node A  │ ←── 통신 불가 ──→ │ Node B  │
│ x = 10  │                   │ x = 5   │
└─────────┘                   └─────────┘
     ↑                              ↑
  사용자 1                       사용자 2
  (x = 10 씀)                   (x 읽기 요청)

선택지 1: 일관성 우선 (CP)
→ Node B: "Node A와 통신 불가, 정확한 값을 모름"
→ 응답 거부 (에러 반환)
→ 데이터 정확성 보장, 하지만 서비스 불가

선택지 2: 가용성 우선 (AP)
→ Node B: "일단 내가 가진 값을 반환하자"
→ x = 5 반환 (오래된 값)
→ 서비스는 계속, 하지만 데이터 불일치
```

### CP vs AP 시스템 예시 ◑

| 유형 | 특징 | 적합한 경우 | 시스템 예시 |
|------|------|------------|------------|
| **CP** | 네트워크 분할 시 일부 요청 거부 | 금융, 재고 관리, 예약 시스템 | HBase, MongoDB, Zookeeper, etcd |
| **AP** | 오래된 데이터라도 항상 응답 | 소셜 미디어, 검색, 추천 | Cassandra, DynamoDB, CouchDB |

**실제 사례**

```
은행 계좌 잔액 (CP 필요):
- 잔액 100만원, 두 ATM에서 동시에 100만원 출금 시도
- 일관성 없으면: 둘 다 성공 → 200만원 인출 (큰 문제!)
- 네트워크 문제 시 차라리 "서비스 불가" 메시지가 나음

인스타그램 좋아요 수 (AP 허용):
- 정확한 숫자보다 서비스 가용성이 중요
- 잠시 동안 다른 숫자가 보여도 치명적이지 않음
- "좋아요 1,234개" vs "좋아요 1,235개" → 큰 문제 아님
```

### PACELC 정리 ◑

CAP 정리를 확장한 모델이다. **네트워크가 정상일 때**의 트레이드오프도 고려한다.

```
PACELC:
if (Partition)
    choose between Availability and Consistency
else (정상 상황)
    choose between Latency and Consistency

파티션 시:     A vs C
정상 시:       L vs C
```

| 시스템 | 파티션 시 | 정상 시 | 설명 |
|--------|----------|---------|------|
| DynamoDB | PA | EL | 가용성 우선, 낮은 지연 |
| Cassandra | PA | EL | 가용성 우선, 낮은 지연 |
| HBase | PC | EC | 일관성 우선, 높은 지연 허용 |
| MongoDB | PC | EC | 일관성 우선 |

---

## 3. 일관성 모델 (Consistency Models) ◑

### ⭕ 강한 일관성과 최종 일관성의 차이는?

**강한 일관성 (Strong Consistency)**

쓰기가 완료되면, 이후 모든 읽기는 **반드시** 그 값을 반환한다.

```
시간 →

Client A:  ──WRITE(x=10)──────────────────→
                    │
                    ▼ (쓰기 완료 확인)

Client B:  ─────────────READ(x)───────────→
                         │
                         ▼ 반드시 10 반환

마치 하나의 서버에 접근하는 것처럼 동작
```

**최종 일관성 (Eventual Consistency)**

쓰기 후 **언젠가는** 모든 노드가 같은 값을 가진다. 하지만 **언제**인지는 보장하지 않는다.

```
시간 →

Client A:  ──WRITE(x=10)──────────────────→
                    │
                    ▼ (쓰기 완료, 복제 시작)

Client B:  ─────────────READ(x)───────────→
                         │
                         ▼ 5 반환 (아직 복제 안 됨)

                         .
                         .

Client B:  ─────────────────────READ(x)───→
                                    │
                                    ▼ 10 반환 (이제 복제됨)
```

**비교**

| 구분 | 강한 일관성 | 최종 일관성 |
|------|------------|------------|
| 읽기 보장 | 항상 최신 값 | 오래된 값 가능 |
| 성능 | 느림 (동기화 대기) | 빠름 |
| 가용성 | 낮음 | 높음 |
| 구현 | 복잡 | 상대적으로 단순 |
| 사용 예 | 은행, 예약 시스템 | SNS, 검색 결과 |

### 일관성 모델 스펙트럼 ◑

```
강함 ◀────────────────────────────────────────────▶ 약함

Linearizability  Sequential    Causal      Eventual
(선형화 가능성)   Consistency   Consistency  Consistency
                 (순차 일관성)  (인과 일관성)  (최종 일관성)

├── 정확성 높음, 성능 낮음 ──────────────────────────┤
├────────────────────────── 정확성 낮음, 성능 높음 ──┤
```

**인과 일관성 (Causal Consistency)**

인과 관계가 있는 연산의 순서만 보장한다. 독립적인 연산은 순서가 달라도 됨.

```
인과 관계 예시:
1. Alice가 "오늘 날씨 좋다" 포스팅
2. Bob이 그 포스팅에 "동의!" 댓글

→ 모든 사용자는 1을 먼저, 2를 나중에 봐야 함
→ 2가 1보다 먼저 보이면 이상함 ("동의!" 댓글만 보임)

인과 관계 없는 예시:
1. Alice가 포스팅
2. Charlie가 (별개의) 포스팅

→ 순서가 달라도 됨. 각각 독립적인 이벤트
```

### 쿼럼 (Quorum) 기반 일관성 ◑

복제된 데이터의 일관성을 보장하는 수학적 방법이다.

```
N = 총 복제본 수
W = 쓰기 성공에 필요한 복제본 수
R = 읽기 성공에 필요한 복제본 수

강한 일관성 조건: W + R > N
```

**왜 W + R > N이면 강한 일관성인가?**

```
예시: N=3 (복제본 3개)

Case 1: W=2, R=2 (W+R=4 > 3) → 강한 일관성 ✓
┌───────────┬───────────┬───────────┐
│  Node 1   │  Node 2   │  Node 3   │
│    ✓      │    ✓      │    ✗      │   ← 쓰기 (2개 성공)
│    ✓      │    ✗      │    ✓      │   ← 읽기 (2개 조회)
└───────────┴───────────┴───────────┘
                ↓
        반드시 최소 1개의 최신 데이터를 읽음

Case 2: W=1, R=1 (W+R=2 < 3) → 최종 일관성
┌───────────┬───────────┬───────────┐
│  Node 1   │  Node 2   │  Node 3   │
│    ✓      │    ✗      │    ✗      │   ← 쓰기 (1개만 성공)
│    ✗      │    ✓      │    ✗      │   ← 읽기 (다른 노드 조회)
└───────────┴───────────┴───────────┘
                ↓
        오래된 데이터를 읽을 수 있음
```

**설정 예시**

| 설정 | W | R | 특징 |
|------|---|---|------|
| 강한 일관성 | 2 | 2 | 쓰기/읽기 모두 느림, 정확함 |
| 빠른 읽기 | 3 | 1 | 쓰기 느림, 읽기 빠름 |
| 빠른 쓰기 | 1 | 3 | 쓰기 빠름, 읽기 느림 |

---

## 4. 합의 알고리즘 (Consensus Algorithms) ◑

### 합의 문제란? ◑

분산 시스템에서 **모든 노드가 하나의 값에 동의**해야 하는 상황이 있다.

```
합의가 필요한 상황들:

1. 리더 선출: "누가 리더인가?"
   - 모든 노드가 같은 리더를 인식해야 함

2. 분산 락: "이 리소스를 누가 사용할 수 있나?"
   - 동시에 한 노드만 락을 획득해야 함

3. 원자적 브로드캐스트: "모든 노드에 같은 순서로 메시지 전달"
   - 로그 복제, 상태 머신 복제
```

**합의의 어려움**

```
노드 3개가 "리더"를 선출하려 할 때:

정상 상황:
Node A: "나를 리더로!" → Node B, C가 동의 → A가 리더

장애 상황:
Node A: "나를 리더로!" → 메시지 전송 중 A 장애
Node B: "A 응답 없음, 나를 리더로!"
Node C: "A가 리더 아닌가? B가 리더인가?"

→ 네트워크 지연, 노드 장애, 메시지 유실 상황에서
   어떻게 합의에 도달할 것인가?
```

### Paxos 알고리즘 ◑

Leslie Lamport가 1989년 발표한 최초의 실용적인 합의 알고리즘이다. 하지만 **이해하기 매우 어렵다**는 문제가 있다.

**Paxos 기본 개념**

```
역할:
- Proposer: 값을 제안
- Acceptor: 값을 수락/거부
- Learner: 최종 결정된 값을 학습

2단계 프로토콜:
Phase 1 (Prepare):
  Proposer → Acceptor: "제안 번호 N으로 제안해도 될까?"
  Acceptor → Proposer: "OK" 또는 "이미 더 높은 번호 봤음"

Phase 2 (Accept):
  Proposer → Acceptor: "제안 번호 N으로 값 V 수락해줘"
  Acceptor → Proposer: "수락" (과반수가 수락하면 합의 완료)
```

Paxos는 정확하지만 복잡해서 실제 구현이 어렵다. 이를 해결하기 위해 Raft가 등장했다.

### ⭕ Raft 알고리즘이란?

Raft는 2013년 Diego Ongaro가 발표한 합의 알고리즘으로, **Paxos와 동등한 기능을 더 이해하기 쉽게** 설계했다.

**Raft의 핵심 아이디어: 리더 기반**

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│     ┌──────────┐                                    │
│     │  Leader  │  ← 모든 쓰기 요청 처리             │
│     └────┬─────┘                                    │
│          │                                          │
│    ┌─────┼─────┐                                    │
│    ▼     ▼     ▼                                    │
│ ┌──────┐ ┌──────┐ ┌──────┐                         │
│ │Follow│ │Follow│ │Follow│  ← 리더의 로그를 복제    │
│ │ er 1 │ │ er 2 │ │ er 3 │                         │
│ └──────┘ └──────┘ └──────┘                         │
│                                                     │
└─────────────────────────────────────────────────────┘

1. 클라이언트 → 리더: 요청
2. 리더 → 팔로워: 로그 복제
3. 과반수 복제 확인 → 커밋
4. 리더 → 클라이언트: 응답
```

**Raft의 세 가지 하위 문제**

**1. 리더 선출 (Leader Election)**

```
노드 상태:
- Follower: 기본 상태, 리더의 명령 수행
- Candidate: 리더가 되려고 시도 중
- Leader: 선출된 리더

선출 과정:
1. 팔로워가 리더로부터 heartbeat를 못 받음 (타임아웃)
2. Candidate로 전환, 자신에게 투표
3. 다른 노드들에게 투표 요청
4. 과반수 득표 → Leader 됨

┌──────────┐  타임아웃   ┌───────────┐  과반수 득표  ┌────────┐
│ Follower │ ─────────→ │ Candidate │ ───────────→ │ Leader │
└──────────┘            └───────────┘              └────────┘
     ↑                        │                        │
     │                        │ 다른 리더 발견         │
     │                        ▼                        │
     └────────────────────────────────────────────────┘
```

**2. 로그 복제 (Log Replication)**

```
리더가 모든 쓰기를 로그에 기록하고 팔로워에게 복제:

Leader Log:    [1] [2] [3] [4] [5]
                │   │   │   │   │
                ▼   ▼   ▼   ▼   ▼
Follower 1:    [1] [2] [3] [4] [5]  ✓ 동기화 완료
Follower 2:    [1] [2] [3] [4]      (아직 복제 중)
Follower 3:    [1] [2] [3] [4] [5]  ✓ 동기화 완료

[5]가 과반수(3/5)에 복제됨 → 커밋 가능
```

**3. 안전성 (Safety)**

- 커밋된 로그는 절대 손실되지 않음
- 새 리더는 반드시 모든 커밋된 로그를 가짐
- 같은 인덱스에 다른 로그가 존재할 수 없음

**Raft 임기(Term) 개념**

```
Term은 "선거 기간"과 같음. 각 Term에는 최대 1명의 리더.

Term 1         Term 2         Term 3
┌──────────────┬──────────────┬──────────────→
│              │              │
│ Leader: A    │ Leader: B    │ Leader: C
│              │              │
│ A 장애 발생 ─→│              │
└──────────────┴──────────────┴──────────────→

Term 번호가 높을수록 "더 최신"
낮은 Term의 메시지는 무시
```

### Paxos vs Raft 비교 ◑

| 구분 | Paxos | Raft |
|------|-------|------|
| 이해 난이도 | 매우 어려움 | 비교적 쉬움 |
| 리더 | 없을 수도 있음 | 항상 필요 |
| 구현 복잡도 | 높음 | 낮음 |
| 실사용 | Chubby, Spanner | etcd, Consul, CockroachDB |

**Raft를 사용하는 이유**
> "Paxos를 이해했다고 주장하는 사람 중 실제로 이해한 사람은 거의 없다"
> — Raft 논문

---

## 5. 데이터 분산 (Data Distribution) ◑

### 샤딩 (Sharding) ◑

데이터를 **여러 노드에 나누어 저장**하는 기법이다. 각 조각을 **샤드(Shard)** 라고 한다.

```
단일 DB의 한계:
┌─────────────────────────────────┐
│           Database              │
│   10억 개 레코드, 10TB          │
│   읽기/쓰기 병목               │
└─────────────────────────────────┘

샤딩 후:
┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐
│  Shard 1   │ │  Shard 2   │ │  Shard 3   │ │  Shard 4   │
│ A-G 사용자 │ │ H-N 사용자 │ │ O-T 사용자 │ │ U-Z 사용자 │
│ 2.5억 개   │ │ 2.5억 개   │ │ 2.5억 개   │ │ 2.5억 개   │
└────────────┘ └────────────┘ └────────────┘ └────────────┘

각 샤드가 독립적으로 읽기/쓰기 처리 → 전체 처리량 4배
```

**샤딩 전략**

| 방식 | 설명 | 장점 | 단점 |
|------|------|------|------|
| **해시 샤딩** | `shard = hash(key) % N` | 균등 분산 | 범위 쿼리 어려움 |
| **범위 샤딩** | 키 범위별로 샤드 결정 | 범위 쿼리 용이 | 핫스팟 위험 |
| **지리 샤딩** | 지역별로 샤드 결정 | 지역성 활용 | 글로벌 쿼리 어려움 |

**핫스팟 문제**

```
범위 샤딩의 문제:
user_id 1-1000000    → Shard 1  (신규 가입자 폭주, 과부하!)
user_id 1000001-2000000 → Shard 2  (한산)
user_id 2000001-3000000 → Shard 3  (한산)

해결책:
- 해시 샤딩으로 변경
- 핫 샤드를 더 쪼개기
- 캐싱 적용
```

### Consistent Hashing (일관된 해싱) ◑

일반 해시 샤딩의 문제: 노드 추가/제거 시 **거의 모든 데이터 재배치**

```
hash(key) % 3  →  서버 3대일 때 분배

서버 4대로 변경하면?
hash(key) % 4  →  거의 모든 키의 위치 변경!

키 "user123"의 해시값: 17
3대일 때: 17 % 3 = 2 (서버 2)
4대일 때: 17 % 4 = 1 (서버 1) ← 위치 변경!
```

**Consistent Hashing의 해결책**

```
원형 해시 공간 (0 ~ 2^32):

                    0
                    │
           N3 ●─────┼─────● N1
                   ╱│╲
                  ╱ │ ╲
                 ╱  │  ╲
                ●───┼───●
               N4   │   N2
                    │
                  2^32/2

키의 해시값 위치에서 시계방향으로 첫 번째 노드에 저장

노드 추가 시:
- 새 노드와 이전 노드 사이의 키만 이동
- 나머지 키는 그대로!
```

**가상 노드 (Virtual Nodes)**

물리 노드 하나를 여러 가상 노드로 분산 배치하여 부하를 균등하게 만든다.

```
가상 노드 없이:
N1 ●───────────────● N2
      N1이 넓은 범위 담당 (불균형)

가상 노드 사용:
N1a ●─── N2a ●─── N1b ●─── N2b ●─── N1c ●─── N2c ●
      균등하게 분산
```

### 복제 (Replication) ◑

동일한 데이터를 **여러 노드에 복사**하여 저장한다.

**복제의 목적**
1. **장애 허용**: 한 노드 장애 시 다른 노드에서 서비스
2. **읽기 성능**: 여러 노드에서 병렬 읽기
3. **지리적 분산**: 사용자와 가까운 노드에서 서비스

**복제 토폴로지**

```
1. 단일 리더 (Single Leader):
   ┌────────┐
   │ Leader │ ← 모든 쓰기
   └───┬────┘
       │ 복제
   ┌───┴───┐
   ▼       ▼
┌──────┐ ┌──────┐
│Follow│ │Follow│ ← 읽기만
└──────┘ └──────┘

장점: 단순, 일관성 보장 쉬움
단점: 리더 병목, 쓰기 지연

2. 다중 리더 (Multi-Leader):
┌────────┐     ┌────────┐
│Leader A│ ←──→│Leader B│  (서로 복제)
└───┬────┘     └───┬────┘
    │              │
    ▼              ▼
 팔로워들        팔로워들

장점: 쓰기 성능 향상, 지역별 리더
단점: 충돌 해결 복잡

3. 리더 없음 (Leaderless):
┌──────┐  ┌──────┐  ┌──────┐
│Node 1│  │Node 2│  │Node 3│
└──┬───┘  └──┬───┘  └──┬───┘
   │         │         │
   └─────────┼─────────┘
   모든 노드에 쓰기 (쿼럼)

장점: 단일 장애점 없음
단점: 쿼럼 관리, 충돌 해결
```

---

## 6. 메시지 큐 (Message Queue) ◑

### ⭕ 메시지 큐를 사용하는 이유는?

메시지 큐는 **생산자(Producer)** 와 **소비자(Consumer)** 사이에서 **메시지를 임시 저장**하는 중간 계층이다.

```
메시지 큐 없이 (동기 통신):
┌──────────┐      ┌──────────┐
│ Service A│ ────→│ Service B│
└──────────┘      └──────────┘
       │                │
       │    B가 느리면   │
       │    A도 대기     │
       │                │
       └────── 강한 결합 ─┘

메시지 큐 사용 (비동기 통신):
┌──────────┐      ┌─────────────┐      ┌──────────┐
│ Service A│ ────→│ Message Queue│ ────→│ Service B│
└──────────┘      └─────────────┘      └──────────┘
       │                                     │
       │    A는 큐에 넣고 바로 반환            │
       │    B는 자기 속도로 처리              │
       │                                     │
       └───────────── 느슨한 결합 ────────────┘
```

**메시지 큐의 이점**

| 이점 | 설명 | 예시 |
|------|------|------|
| **비동기 처리** | 즉시 응답, 나중에 처리 | 이메일 발송, 이미지 처리 |
| **부하 분산** | 트래픽 피크 시 버퍼 역할 | 티켓팅, 이벤트 처리 |
| **느슨한 결합** | 서비스 간 독립성 | MSA 환경 |
| **장애 허용** | 일시적 장애에도 메시지 보존 | 결제 시스템 |

**실제 사용 사례**

```
이커머스 주문 처리:

1. 동기식 (메시지 큐 없이):
   주문 → 결제 → 재고 → 배송 → 알림 → 응답
   (각 단계가 느리면 전체가 느림, 한 곳 장애 시 전체 실패)

2. 비동기식 (메시지 큐 사용):
   주문 → [주문 완료 응답] → 큐에 메시지
                              ├→ 결제 서비스 (큐에서 가져가 처리)
                              ├→ 재고 서비스
                              ├→ 배송 서비스
                              └→ 알림 서비스

   (즉시 응답, 각 서비스는 독립적으로 처리)
```

### 메시지 전달 보장 수준 ◑

**At-most-once (최대 1회)**
```
Producer → Queue → Consumer
             ↓
         메시지 전달 후 즉시 삭제

문제: Consumer가 처리 실패해도 재전송 없음
     메시지 유실 가능

사용: 로그, 모니터링 (유실 허용)
```

**At-least-once (최소 1회)**
```
Producer → Queue → Consumer
             ↓
         Consumer가 ACK 보낼 때까지 보관
         타임아웃 시 재전송

문제: Consumer가 처리 후 ACK 전에 죽으면?
     재전송 → 중복 처리

해결: Consumer가 멱등성 있게 구현
     (같은 메시지 여러 번 처리해도 결과 동일)

사용: 대부분의 경우 (결제, 주문)
```

**Exactly-once (정확히 1회)**
```
이론적으로는 불가능에 가까움

실제 구현:
- At-least-once + Consumer 멱등성
- 트랜잭션 지원 (Kafka의 Exactly-Once Semantics)
- 분산 트랜잭션 (2PC)

사용: 금융 시스템 (매우 중요한 경우)
```

### 주요 메시지 큐 시스템 비교 ◑

| 시스템 | 특징 | 적합한 용도 |
|--------|------|------------|
| **Kafka** | 높은 처리량, 로그 기반, 순서 보장, 데이터 보존 | 로그 수집, 이벤트 스트리밍, 실시간 분석 |
| **RabbitMQ** | AMQP 프로토콜, 유연한 라우팅, 다양한 패턴 | 작업 큐, 복잡한 라우팅 |
| **Amazon SQS** | 관리형, 무한 확장, 간단한 API | 간단한 비동기 처리 |
| **Redis Streams** | 인메모리, 매우 빠름 | 실시간 처리, 채팅 |

**Kafka 아키텍처**

```
┌─────────────────────────────────────────────────────────┐
│                      Kafka Cluster                      │
│                                                         │
│  Topic: "orders"                                        │
│  ┌─────────────┬─────────────┬─────────────┐           │
│  │ Partition 0 │ Partition 1 │ Partition 2 │           │
│  │ [0,1,2,3,4] │ [0,1,2,3]   │ [0,1,2,3,4,5]│          │
│  └──────┬──────┴──────┬──────┴──────┬──────┘           │
│         │             │             │                   │
└─────────┼─────────────┼─────────────┼───────────────────┘
          │             │             │
    Consumer      Consumer      Consumer
    Group A       Group A       Group A
    (각 파티션을 한 컨슈머가 담당)

특징:
- 메시지가 디스크에 저장 (재처리 가능)
- 파티션 내 순서 보장
- 컨슈머 그룹으로 병렬 처리
```

---

## 7. 로드 밸런싱 (Load Balancing) ◑

### 로드 밸런서란? ◑

요청을 **여러 서버에 분산**하여 부하를 균등하게 만드는 컴포넌트이다.

```
로드 밸런서 없이:
         ┌─────────┐
Client ──┤ Server 1│ ← 모든 요청이 하나로
         └─────────┘

로드 밸런서 사용:
                        ┌─────────┐
         ┌─────────┐ ──→│ Server 1│
Client ──┤   LB    │ ──→│ Server 2│
         └─────────┘ ──→│ Server 3│
                        └─────────┘
```

### 로드 밸런싱 알고리즘 ◑

**1. 라운드 로빈 (Round Robin)**
```
요청 순서대로 돌아가면서 분배:
요청 1 → Server 1
요청 2 → Server 2
요청 3 → Server 3
요청 4 → Server 1
...

장점: 단순, 균등 분배
단점: 서버 성능 차이 무시
```

**2. 가중 라운드 로빈 (Weighted Round Robin)**
```
서버 성능에 따라 가중치 부여:
Server 1 (weight=3): 요청 1, 2, 3
Server 2 (weight=1): 요청 4
Server 3 (weight=2): 요청 5, 6
...

장점: 서버 성능 반영
단점: 실시간 부하 상태 미반영
```

**3. 최소 연결 (Least Connections)**
```
현재 연결 수가 가장 적은 서버로:
Server 1: 10개 연결
Server 2: 5개 연결  ← 새 요청 할당
Server 3: 8개 연결

장점: 실시간 부하 반영
단점: 연결 수 추적 오버헤드
```

**4. IP 해시 (IP Hash)**
```
클라이언트 IP로 서버 결정:
hash(client_IP) % 서버수

같은 클라이언트 → 항상 같은 서버

장점: 세션 유지 (Sticky Session)
단점: 부하 불균형 가능
```

### L4 vs L7 로드 밸런싱 ◑

```
OSI 계층:
┌─────────────────────────────────────┐
│ L7: Application   (HTTP, HTTPS)     │ ← L7 로드 밸런서
├─────────────────────────────────────┤
│ L6: Presentation                    │
├─────────────────────────────────────┤
│ L5: Session                         │
├─────────────────────────────────────┤
│ L4: Transport     (TCP, UDP)        │ ← L4 로드 밸런서
├─────────────────────────────────────┤
│ L3: Network       (IP)              │
└─────────────────────────────────────┘
```

| 구분 | L4 로드 밸런서 | L7 로드 밸런서 |
|------|---------------|---------------|
| 동작 계층 | 전송 계층 (TCP/UDP) | 응용 계층 (HTTP) |
| 분배 기준 | IP 주소, 포트 | URL, 헤더, 쿠키, 콘텐츠 |
| 속도 | 빠름 (패킷 레벨) | 느림 (내용 파싱 필요) |
| 유연성 | 낮음 | 높음 (스마트 라우팅) |
| 예시 | AWS NLB, HAProxy (TCP) | AWS ALB, Nginx |

**L7 로드 밸런싱 활용 예**

```
URL 기반 라우팅:
/api/*     → API 서버 클러스터
/images/*  → 이미지 서버 클러스터
/static/*  → CDN
/*         → 웹 서버 클러스터

헤더 기반 라우팅:
User-Agent: *Mobile* → 모바일 서버
Accept-Language: ko  → 한국 서버
```

---

## 8. 분산 시스템 패턴 ◑

### Circuit Breaker (서킷 브레이커) ◑

전기의 차단기처럼, **연속 실패 시 요청을 차단**하여 장애 확산을 방지한다.

```
왜 필요한가?

Service A → Service B (장애 상황)
     │           ↓
     │      응답 없음, 타임아웃
     │           ↓
     │      A가 계속 기다림
     │           ↓
     │      A도 느려짐 (리소스 고갈)
     │           ↓
     └────→ 장애가 A로 전파 (Cascade Failure)
```

**Circuit Breaker 상태**

```
┌────────────────────────────────────────────────────────────┐
│                                                            │
│     ┌──────────┐   실패 임계치 초과   ┌──────────┐        │
│     │  Closed  │ ───────────────────→│   Open   │        │
│     │ (정상)   │                     │ (차단)   │        │
│     └────┬─────┘                     └────┬─────┘        │
│          │                                │               │
│          │        타임아웃 후            │               │
│          │        ┌─────────────┐        │               │
│          │        │ Half-Open   │←───────┘               │
│          │        │ (시험)      │                        │
│          │        └──────┬──────┘                        │
│          │               │                               │
│          │  성공         │ 실패                          │
│          │←──────────────┤──────────────→ Open          │
│                                                            │
└────────────────────────────────────────────────────────────┘

상태 설명:
- Closed: 정상 동작, 실패 횟수 카운트
- Open: 요청 즉시 실패 반환 (B 호출 안 함)
- Half-Open: 일부 요청만 허용하여 복구 확인
```

**구현 예시 (의사 코드)**

```python
class CircuitBreaker:
    def __init__(self):
        self.failure_count = 0
        self.state = "CLOSED"
        self.last_failure_time = None

    def call(self, func):
        if self.state == "OPEN":
            if time.now() - self.last_failure_time > TIMEOUT:
                self.state = "HALF_OPEN"
            else:
                raise CircuitOpenException("Service unavailable")

        try:
            result = func()
            self.on_success()
            return result
        except Exception:
            self.on_failure()
            raise

    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"

    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.now()
        if self.failure_count >= THRESHOLD:
            self.state = "OPEN"
```

### Saga 패턴 ◑

분산 환경에서 **여러 서비스에 걸친 트랜잭션**을 관리하는 패턴이다.

**문제: 분산 트랜잭션**

```
이커머스 주문:
1. 주문 서비스: 주문 생성
2. 결제 서비스: 결제 처리
3. 재고 서비스: 재고 차감
4. 배송 서비스: 배송 시작

3단계에서 실패하면?
→ 1, 2번을 롤백해야 함
→ 각 서비스가 독립적인 DB를 가진 MSA에서는?
→ 일반적인 DB 트랜잭션 불가능
```

**Saga의 해결책: 보상 트랜잭션**

```
각 단계마다 "되돌리는 방법"을 정의:

T1: 주문 생성     ←→ C1: 주문 취소
T2: 결제 처리     ←→ C2: 환불
T3: 재고 차감     ←→ C3: 재고 복구
T4: 배송 시작     ←→ C4: 배송 취소

실행: T1 → T2 → T3 (실패!)
보상: C2 → C1 (역순으로 롤백)
```

**Saga 구현 방식**

```
1. Choreography (이벤트 기반):
   각 서비스가 이벤트를 발행하고 구독

   주문 서비스 ──"주문 생성됨"──→ 결제 서비스
                               │
                               ▼
   재고 서비스 ←──"결제 완료됨"────┘
        │
        ▼
   "재고 부족" ──→ 결제 서비스 ──"환불 처리"──→ 주문 서비스

   장점: 느슨한 결합
   단점: 흐름 파악 어려움, 디버깅 어려움

2. Orchestration (중앙 조정자):
   중앙 조정자(Orchestrator)가 전체 흐름 관리

   ┌─────────────────────────────────────────┐
   │            Saga Orchestrator            │
   │                                         │
   │  T1 → T2 → T3 → T4                     │
   │             ↓                           │
   │         실패 시 C2 → C1                │
   └─────────────────────────────────────────┘
        │      │      │      │
        ▼      ▼      ▼      ▼
      주문   결제   재고   배송

   장점: 흐름 파악 쉬움, 중앙 관리
   단점: Orchestrator가 단일 장애점
```

### 2PC (Two-Phase Commit) ◑

분산 트랜잭션의 **원자성(Atomicity)** 을 보장하는 프로토콜이다.

```
Coordinator가 모든 참여자의 준비 상태를 확인 후 커밋

Phase 1: Prepare (준비)
┌─────────────┐
│ Coordinator │
└──────┬──────┘
       │ "준비됐니?"
       ├─────────────────────┬─────────────────────┐
       ▼                     ▼                     ▼
  ┌─────────┐          ┌─────────┐          ┌─────────┐
  │ Node A  │          │ Node B  │          │ Node C  │
  │  "OK"   │          │  "OK"   │          │  "OK"   │
  └─────────┘          └─────────┘          └─────────┘

Phase 2: Commit (커밋)
┌─────────────┐
│ Coordinator │  ← 모두 OK → "커밋해!"
└──────┬──────┘     하나라도 NO → "롤백해!"
       │
       ├─────────────────────┬─────────────────────┐
       ▼                     ▼                     ▼
  ┌─────────┐          ┌─────────┐          ┌─────────┐
  │ Node A  │          │ Node B  │          │ Node C  │
  │ Commit  │          │ Commit  │          │ Commit  │
  └─────────┘          └─────────┘          └─────────┘
```

**2PC의 문제점**

```
Coordinator 장애 시:
- Phase 1 완료 후 Coordinator 죽음
- 참여자들: "커밋? 롤백? 모름..."
- 블로킹 상태로 대기 (리소스 잠금)

성능:
- 모든 노드가 동기적으로 응답해야 함
- 가장 느린 노드가 전체 성능 결정
```

**Saga vs 2PC**

| 구분 | Saga | 2PC |
|------|------|-----|
| 일관성 | 최종 일관성 | 강한 일관성 |
| 롤백 | 보상 트랜잭션 | 실제 롤백 |
| 가용성 | 높음 | 낮음 (블로킹) |
| 복잡도 | 보상 로직 필요 | 상대적으로 단순 |
| 적합 | MSA, 장시간 트랜잭션 | 짧은 트랜잭션, 강한 일관성 필요 |

---

## 9. 분산 캐시 (Distributed Cache) ◑

### 캐시 전략 ◑

**1. Cache-Aside (Lazy Loading)**

```
애플리케이션이 직접 캐시를 관리:

읽기:
App ──→ Cache 확인 ──→ 있음 → 반환
                   └──→ 없음 → DB 조회 → Cache에 저장 → 반환

쓰기:
App ──→ DB에 쓰기 ──→ Cache 삭제 (또는 갱신)

장점: 필요한 데이터만 캐싱, 캐시 장애에도 동작
단점: 캐시 미스 시 지연, 캐시-DB 불일치 가능
```

**2. Read-Through**

```
캐시가 DB 조회를 대행:

App ──→ Cache ──→ 있음 → 반환
              └──→ 없음 → Cache가 DB 조회 → 저장 → 반환

장점: 애플리케이션 로직 단순화
단점: 캐시 시스템이 DB 접근 로직 필요
```

**3. Write-Through**

```
쓰기 시 캐시와 DB 동시 갱신:

App ──→ Cache에 쓰기 ──→ Cache가 DB에 쓰기
                     └──→ 완료 후 응답

장점: 캐시-DB 일관성 보장
단점: 쓰기 지연 (두 번 쓰기)
```

**4. Write-Behind (Write-Back)**

```
캐시에만 쓰고, 나중에 DB 반영:

App ──→ Cache에 쓰기 ──→ 즉시 응답
                     │
         (비동기로 나중에)
                     ▼
                  DB에 쓰기

장점: 쓰기 성능 매우 빠름
단점: 캐시 장애 시 데이터 유실, 일관성 위험
```

### 캐시 무효화 전략 ◑

| 전략 | 설명 | 장단점 |
|------|------|--------|
| **TTL** | 만료 시간 기반 | 단순, 오래된 데이터 가능 |
| **이벤트 기반** | 데이터 변경 시 무효화 | 정확, 이벤트 시스템 필요 |
| **버전 기반** | 버전 번호로 확인 | 유연, 버전 관리 필요 |

**캐시 스탬피드 (Cache Stampede)**

```
문제:
인기 있는 데이터의 TTL 만료
         ↓
동시에 수천 요청이 캐시 미스
         ↓
모두 DB로 쿼리 → DB 과부하

해결책:
1. 락 사용: 한 요청만 DB 조회, 나머지 대기
2. 배경 갱신: TTL 만료 전에 미리 갱신
3. 랜덤 TTL: 만료 시점 분산
```

---

## 10. 시스템 설계 면접 접근법 ◑

### 설계 단계 ◑

```
1. 요구사항 명확화 (5분)
   - 기능적 요구사항: "무엇을 해야 하는가?"
   - 비기능적 요구사항: "얼마나 빠르게? 얼마나 크게?"
   - 제약 조건: 예산, 기술 스택

2. 규모 추정 (5분)
   - 사용자 수, DAU, MAU
   - QPS (Queries Per Second)
   - 저장 용량
   - 네트워크 대역폭

3. 고수준 설계 (10-15분)
   - 주요 컴포넌트 식별
   - 데이터 흐름 정의
   - API 설계

4. 상세 설계 (10-15분)
   - 핵심 컴포넌트 깊이 있게
   - 데이터 모델
   - 알고리즘

5. 병목 식별 및 해결 (5-10분)
   - 확장성
   - 장애 대응
   - 모니터링
```

### 규모 추정 빠른 계산 ◑

```
유용한 숫자들:
- 1일 = 86,400초 ≈ 100,000초
- 1MB = 10^6 bytes
- 1GB = 10^9 bytes
- 1TB = 10^12 bytes

예시: 트위터 타임라인
- DAU: 3억 명
- 하루 평균 트윗 조회: 100회/사용자
- QPS = 3억 × 100 / 86,400 ≈ 350,000 QPS
- 피크: 평균의 3배 ≈ 100만 QPS
```

### 대표 설계 문제와 핵심 포인트 ◑

| 문제 | 핵심 컴포넌트 | 주요 고려사항 |
|------|--------------|--------------|
| **URL 단축기** | 해시 생성, KV 스토어, 리다이렉션 | 해시 충돌, 분산 ID 생성 |
| **뉴스피드** | 팬아웃, 타임라인 캐시, 랭킹 | Push vs Pull, 핫 유저 처리 |
| **채팅 시스템** | WebSocket, 메시지 큐, 상태 서버 | 읽음 상태, 그룹 채팅, 오프라인 처리 |
| **Rate Limiter** | Token Bucket, 분산 카운터 | 분산 환경 동기화, 슬라이딩 윈도우 |
| **검색 자동완성** | Trie, 분산 캐시, 랭킹 | 실시간 갱신, 개인화 |

---

## 면접 대비 체크리스트 ◑

- [ ] CAP 정리 설명과 CP/AP 시스템 예시
- [ ] 강한 일관성 vs 최종 일관성: 언제 어떤 것을 선택하는가?
- [ ] Raft 합의 알고리즘: 리더 선출, 로그 복제 과정
- [ ] 샤딩 전략과 Consistent Hashing의 동작 원리
- [ ] 메시지 큐의 역할과 전달 보장 수준 (At-least-once, Exactly-once)
- [ ] 로드 밸런싱 알고리즘과 L4/L7 차이
- [ ] Circuit Breaker 패턴의 상태 전이
- [ ] Saga 패턴: Choreography vs Orchestration
- [ ] 캐시 전략 (Cache-Aside, Write-Through 등)
- [ ] 시스템 설계 면접 접근 단계

