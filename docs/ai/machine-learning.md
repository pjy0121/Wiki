# 12. 머신러닝과 딥러닝 (Machine Learning & Deep Learning) ◑

---

## 1. 머신러닝 기초 ◑

### ⭕ 머신러닝이란?

데이터로부터 패턴을 학습하여 예측이나 결정을 수행하는 알고리즘이다.

### 학습 유형 ◑

| 유형 | 설명 | 예시 |
|------|------|------|
| **지도 학습** | 입력-출력 쌍으로 학습 | 분류, 회귀 |
| **비지도 학습** | 레이블 없이 패턴 발견 | 클러스터링, 차원 축소 |
| **강화 학습** | 환경과 상호작용, 보상 최대화 | 게임, 로봇 제어 |

### ⭕ 분류(Classification)와 회귀(Regression)의 차이는?

| 구분 | 분류 | 회귀 |
|------|------|------|
| 출력 | 범주형 (클래스) | 연속형 (숫자) |
| 예시 | 스팸 메일 분류 | 집값 예측 |
| 평가 지표 | Accuracy, F1-Score | MSE, RMSE, R² |

---

## 2. 주요 알고리즘 ◑

### 분류/회귀 알고리즘 ◑

| 알고리즘 | 특징 |
|----------|------|
| 선형 회귀 | 연속값 예측, 가장 기본적인 회귀 |
| 로지스틱 회귀 | 이진 분류, 시그모이드 함수 사용 |
| 결정 트리 | 규칙 기반, 해석 용이 |
| 랜덤 포레스트 | 앙상블, 여러 결정 트리 결합 |
| SVM | 마진 최대화, 고차원에 효과적 |
| k-NN | 거리 기반, 단순함 |

### 비지도 학습 알고리즘 ◑

| 알고리즘 | 용도 |
|----------|------|
| K-Means | 클러스터링 |
| PCA | 차원 축소 |
| DBSCAN | 밀도 기반 클러스터링 |

---

## 3. 모델 평가 ◑

### ⭕ 과적합(Overfitting)이란? 해결 방법은?

모델이 훈련 데이터의 노이즈까지 학습하여 새로운 데이터에 일반화하지 못하는 현상이다.

| 해결 방법 | 설명 |
|-----------|------|
| 정규화 (L1, L2) | 가중치에 페널티 부여 |
| Dropout | 뉴런 랜덤 비활성화 |
| 조기 종료 | 검증 손실 증가 시 학습 중단 |
| 데이터 증강 | 훈련 데이터 다양화 |
| 교차 검증 | 여러 폴드로 검증 |

### 평가 지표 ◑

**분류**

| 지표 | 설명 |
|------|------|
| Accuracy | 전체 정확도 |
| Precision | 예측 양성 중 실제 양성 비율 |
| Recall | 실제 양성 중 예측 양성 비율 |
| F1-Score | Precision과 Recall의 조화 평균 |
| AUC-ROC | 분류기 성능 곡선 아래 면적 |

### ⭕ 혼동 행렬 (Confusion Matrix) ◑

```
                예측
              양성   음성
실제  양성    TP     FN
      음성    FP     TN
```

| 지표 | 계산 | 의미 |
|------|------|------|
| Accuracy | (TP+TN)/(TP+TN+FP+FN) | 전체 정확도 |
| Precision | TP/(TP+FP) | 양성 예측의 정확도 |
| Recall (Sensitivity) | TP/(TP+FN) | 실제 양성 탐지율 |
| Specificity | TN/(TN+FP) | 실제 음성 탐지율 |
| F1-Score | 2×(P×R)/(P+R) | Precision과 Recall 조화평균 |

**언제 어떤 지표를 사용하는가?**
- 스팸 필터: Precision 중요 (정상 메일을 스팸으로 분류하면 안 됨)
- 암 진단: Recall 중요 (실제 암환자를 놓치면 안 됨)
- 데이터 불균형: F1-Score 또는 AUC-ROC 사용

**회귀**

| 지표 | 설명 |
|------|------|
| MSE | 평균 제곱 오차 |
| RMSE | MSE의 제곱근 |
| MAE | 평균 절대 오차 |
| R² | 결정 계수 |

### ⭕ Bias와 Variance의 트레이드오프란?

| 개념 | 설명 |
|------|------|
| Bias (편향) | 모델이 단순해서 생기는 오차 → Underfitting |
| Variance (분산) | 훈련 데이터에 민감 → Overfitting |

좋은 모델은 Bias와 Variance의 균형을 유지한다.

### ⭕ 교차 검증 (Cross-Validation) ◑

데이터를 여러 폴드로 나누어 검증하는 기법이다.

```
K-Fold Cross Validation (K=5):

폴드1: [테스트] [훈련] [훈련] [훈련] [훈련]
폴드2: [훈련] [테스트] [훈련] [훈련] [훈련]
폴드3: [훈련] [훈련] [테스트] [훈련] [훈련]
폴드4: [훈련] [훈련] [훈련] [테스트] [훈련]
폴드5: [훈련] [훈련] [훈련] [훈련] [테스트]

최종 성능 = 5개 폴드 성능의 평균
```

| 유형 | 설명 |
|------|------|
| K-Fold | 데이터를 K개로 분할 |
| Stratified K-Fold | 클래스 비율 유지 (분류에서 권장) |
| Leave-One-Out | K = 데이터 수 (작은 데이터셋에 사용) |

**장점:** 모든 데이터가 훈련과 검증에 사용됨, 과적합 탐지에 효과적

### 데이터 전처리 ◑

| 기법 | 설명 | 공식 |
|------|------|------|
| 정규화 (Normalization) | 0~1 범위로 스케일링 | (x - min) / (max - min) |
| 표준화 (Standardization) | 평균 0, 표준편차 1 | (x - μ) / σ |
| 로그 변환 | 왜곡된 분포 정규화 | log(x) |

**언제 어떤 것을 사용하는가?**
- 정규화: 신경망, 이미지 데이터
- 표준화: SVM, 선형 회귀, 이상치 있는 데이터

### ⭕ 클래스 불균형 처리 ◑

| 방법 | 설명 |
|------|------|
| 오버샘플링 (SMOTE) | 소수 클래스 데이터 합성 생성 |
| 언더샘플링 | 다수 클래스 데이터 축소 |
| 클래스 가중치 | 손실 함수에 가중치 부여 |
| 임계값 조정 | 분류 임계값 조정 |

---

## 4. 딥러닝 기초 ◑

### ⭕ 딥러닝이란?

여러 은닉층을 가진 신경망을 통해 복잡한 패턴을 학습하는 머신러닝의 하위 분야이다.

### 신경망 구조 ◑

```
입력층 → 은닉층(들) → 출력층
```

| 구성 요소 | 설명 |
|-----------|------|
| 뉴런 (노드) | 입력의 가중합 + 활성화 함수 |
| 가중치 (Weight) | 학습되는 파라미터 |
| 편향 (Bias) | 각 뉴런의 오프셋 |
| 활성화 함수 | 비선형성 도입 |

### 활성화 함수 ◑

| 함수 | 특징 |
|------|------|
| Sigmoid | 출력 0~1, 기울기 소실 문제 |
| Tanh | 출력 -1~1 |
| ReLU | max(0, x), 가장 많이 사용 |
| Leaky ReLU | 음수에서도 작은 기울기 |
| Softmax | 다중 분류 출력층 |

### ⭕ 역전파(Backpropagation)란?

출력 오차를 역방향으로 전파하여 가중치를 업데이트하는 알고리즘이다.

```
순전파 → 손실 계산 → 역전파 (체인 룰로 기울기 계산) → 가중치 업데이트
```

### ⭕ 기울기 소실(Vanishing Gradient) 문제란?

깊은 네트워크에서 역전파 시 기울기가 매우 작아져 학습이 어려워지는 현상이다.

| 해결 방법 | 설명 |
|-----------|------|
| ReLU 활성화 함수 | Sigmoid 대신 사용 |
| Batch Normalization | 층의 입력을 정규화 |
| 잔차 연결 (ResNet) | Skip connection |
| LSTM/GRU | RNN의 기울기 소실 해결 |

---

## 5. 주요 신경망 아키텍처 ◑

### CNN (Convolutional Neural Network) ◑

이미지 처리에 특화된 신경망이다.

| 층 | 역할 |
|-----|------|
| Convolution | 필터로 특징 추출 |
| Pooling | 다운샘플링, 공간 크기 축소 |
| Fully Connected | 최종 분류 |

**특징:**
- 파라미터 공유로 효율적
- 공간적 계층 구조 학습
- Translation Invariance

### RNN (Recurrent Neural Network) ◑

순차 데이터 처리를 위한 신경망이다.

| 변형 | 특징 |
|------|------|
| LSTM | 장기 의존성 학습, 게이트 메커니즘 |
| GRU | LSTM 간소화, 더 적은 파라미터 |

**활용:** 자연어 처리, 시계열 예측

### Transformer ◑

Self-Attention 기반의 아키텍처이다. (자세한 내용은 13장 참고)

---

## 6. 학습 기법 ◑

### 경사 하강법 변형 ◑

| 옵티마이저 | 특징 |
|------------|------|
| SGD | 기본, 미니배치 사용 |
| Momentum | 관성 추가, 진동 감소 |
| Adam | Momentum + RMSprop, 가장 많이 사용 |

### 학습률 (Learning Rate) ◑

- 너무 높으면: 발산
- 너무 낮으면: 느린 수렴

**학습률 스케줄링:** 학습 중 학습률 조정 (Step, Cosine, Warmup)

### Batch Normalization ◑

각 층의 입력을 정규화하여 학습을 안정화하고 속도를 높인다.

---

## 7. 전이 학습 ◑

### ⭕ 전이 학습(Transfer Learning)이란?

사전 학습된 모델을 새로운 작업에 적용하는 기법이다.

| 방법 | 설명 |
|------|------|
| Feature Extraction | 사전 학습 층 동결, 새 분류층만 학습 |
| Fine-tuning | 일부/전체 층을 새 데이터로 재학습 |

**장점:** 적은 데이터로 좋은 성능, 학습 시간 단축

---

## 8. 프레임워크 ◑

| 프레임워크 | 특징 |
|------------|------|
| PyTorch | 동적 그래프, 연구에 인기 |
| TensorFlow | 정적 그래프, 프로덕션에 강함 |
| Keras | 고수준 API, 빠른 프로토타이핑 |
| JAX | 함수형, 고성능 연산 |

---

## 면접 대비 체크리스트 ◑

- [ ] 지도/비지도/강화 학습 차이
- [ ] 분류 vs 회귀
- [ ] 혼동 행렬과 Precision/Recall ◑
- [ ] 과적합과 해결 방법
- [ ] Bias-Variance 트레이드오프
- [ ] 교차 검증 (K-Fold) ◑
- [ ] 데이터 정규화 vs 표준화 ◑
- [ ] 클래스 불균형 처리 ◑
- [ ] 역전파와 기울기 소실
- [ ] CNN, RNN 구조와 특징
- [ ] 옵티마이저 종류 (SGD, Adam)
- [ ] 전이 학습

